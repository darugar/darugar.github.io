<!DOCTYPE html>
<html lang="en">
<head>
    <meta charset="UTF-8">
    <meta http-equiv="X-UA-Compatible" content="IE=edge">
    <meta name="viewport" content="width=device-width, initial-scale=1.0">
    <title>A Non-Technical Explanation of ChatGPT - Parand</title>
    <link rel="shortcut icon" type="image/png" href="/theme/images/favico.png"/>

    <script src="https://cdn.tailwindcss.com"></script>
    <link rel="stylesheet" href="https://fonts.googleapis.com/css2?family=Open+Sans">
    <link rel="stylesheet" data-href="https://fonts.googleapis.com/css2?family=EB+Garamond:wght@400;600" data-optimized-fonts="true">
    <link rel="stylesheet" href="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/styles/default.min.css">
    <script src="//cdnjs.cloudflare.com/ajax/libs/highlight.js/11.7.0/highlight.min.js"></script>
    <script>hljs.initHighlightingOnLoad();</script>
    <style>
        body {
          font-family: "EB Garmond",warnock-pro,Palatino,"Palatino Linotype","Palatino LT STD","Book Antiqua",Georgia,serif;
        }
        h1 { 
          font-size: x-large !important;
          font-weight: bold !important;
        }
        h2 { 
          font-size: larger !important;
          font-weight: bold !important;
        }
        div.highlight code {
          border: 1px solid #AAA;
          background-color: #fff !important;
          font-size: medium !important;
        }
        table.dataframe {
          background-color: #fff !important;
          font-family: monospace !important;
        }
        table.dataframe th {
          text-align: left !important;
          padding-right: 5px;
        }
        section.article a {
          color: brown !important;
        }

        .article-summary h1, .article-summary p {
          margin-top: 10px !important;
        }
    </style>

<!-- Google tag (gtag.js) -->
<script async src="https://www.googletagmanager.com/gtag/js?id=G-XGEBWVBEKP"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'G-XGEBWVBEKP');
</script>

<link rel="canonical" href="https://www.parand.com/a-non-technical-explanation-of-chatgpt.html" />

</head>


<body>
  
<div id="root" class="container py-2 mx-auto text-lg bg-gray-100 rounded max-w-6xl">

    <header class="text-gray-100 body-font bg-gray-700">
        <div class="container mx-auto flex flex-wrap p-5 flex-col md:flex-row items-center text-xl bg-[url('/theme/images/header-bg-1-1920.jpg')]">
          <a class="flex title-font font-medium items-center text-gray-100 mb-4 md:mb-0" href="/">
            <img width=180 src="/theme/images/logo-parand.png">
            <span class="ml-5 text-3xl">Standard Deviations</span>
          </a>
          <nav class="md:ml-auto flex flex-wrap items-center justify-center text-2xl">
            <a class="mr-5 hover:text-gray-400" href="/">Blog</a>
            <a class="mr-5 hover:text-gray-400" href="/pages/writings.html">Writings</a>
            <a class="mr-5 hover:text-gray-400" href="/pages/parand-tony-darugar.html">About</a>
          </nav>
        </div>
    </header>

    <section class="text-gray-600 body-font overflow-hidden">
        <div class="container px-5 py-10 mx-auto">
            <div class="-my-8 divide-y-2 divide-gray-100">

<div class="content-article">

  <div class="py-8 row">
    <h1><a href="a-non-technical-explanation-of-chatgpt.html" class="text-3xl font-medium text-gray-900 title-font mb-2">A Non-Technical Explanation of ChatGPT</a></h1>
    <header class="header-article">
        <div class="text-sm">Thu 04 May 2023</div>
    </header>
  </div>

    <section class='article'>
        
        <!-- div class="entry-summary">
            <p>Continuing the series of non-technical explainers, let's figure out how ChatGPT (and in general Large Language Models, 
or LLMs) work. This is part one of the ChatGPT explainer, with part two coming soon.</p>
<p>It's helpful but not necessary to read the <a href="https://www.parand.com/a-completely-non-technical-explanation-of-ai.html">non-technical explanation of AI</a> first, if you feel like …</p>
        </div -->
    
        <div class="entry-content space-y-5">
            <p>Continuing the series of non-technical explainers, let's figure out how ChatGPT (and in general Large Language Models, 
or LLMs) work. This is part one of the ChatGPT explainer, with part two coming soon.</p>
<p>It's helpful but not necessary to read the <a href="https://www.parand.com/a-completely-non-technical-explanation-of-ai.html">non-technical explanation of AI</a> first, if you feel like it read that and come back.</p>
<h1>Fill In The Blank</h1>
<p>Let's play a quick game of fill in the blank:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
to be or not to _____
</blockquote>

<p>Why did you immediately think of the word <em>be</em> to fill in that blank, instead of the word <em>banana</em> or <em>fish</em>? Because you've seen the phrase many times and your brain has learned the most likely next word is <em>be</em>.</p>
<p>Let's do another one:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
rock and ____
</blockquote>

<p>Did you think of the word <em>roll</em>? Why? Because that's the word you see most often following the words <em>rock and</em>.</p>
<p>How about this one:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
I am very _____
</blockquote>

<p>This is less clear - the next word depends on the context.</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
I just ran a marathon. I am very _____
</blockquote>

<p>Perhaps now you would say <em>tired</em>. Or <em>happy</em>, or <em>proud</em>.</p>
<p>Your brain is picking the most likely next word based on the context of the sentence and based on what words it's seen most frequently in that context.</p>
<h1>Aliens and CatGPT</h1>
<p>In an astonishing turn of events a race of alien cats have landed on earth. Since you are <a href="https://www.parand.com/a-completely-non-technical-explanation-of-ai.html">the world's foremost cat expert</a> you have been chosen to communicate with them.</p>
<p><img src="/static/images/alien-cat.png"></p>
<p>Your boss walks in and drops a massive print out of all the alien cat communications on your desk.</p>
<p>"Speak cat!" she commands.</p>
<p>Unfortunately you don't speak alien cat.</p>
<p>You page through the print outs - it looks like pages and pages of gibberish.</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
zoog zeeg zag.
zoog zeeg kaz.
zoog zeeg bah.
rag zoog zeeg.
rag zoog ko.
kaz rag.
zap zoog zeeg.
...
</blockquote>

<h1>Markov to the Rescue</h1>
<p>What to do? You remember your friend Markov who's always talking about languages, words,
and their relationships. Maybe he can help. You show him the alien cat communications and
ask him if he can help you say something in cat.</p>
<p>"Yes!" he exclaims - "We can do this. We will create our response one word at a time, just
by picking the best word to say next, and keep going until we have a sentence"</p>
<p>"But how do we know what the best word to say next is? Don't we need to know what the words 
mean?"</p>
<p>"Nope, we just need to know what word to say next"</p>
<p>"That... doesn't seem like it would work"</p>
<p>"Let me show you", says Markov, and grabs the book on his desk, 
<a href="https://www.gutenberg.org/ebooks/11">Alice in Wonderland</a>.</p>
<p><img src="/static/images/alice-in-wonderland-cat.png"></p>
<p>"We just need to know what word to say next. The best word to say next is simply
the word that shows up most frequently after the word we're looking at. All we
have to do is make a table of how often each word follows another word. We'll call
this a frequency table"</p>
<p>He shows you how to create the frequency table - you just note down how many times each word follows another.
It takes a while to go through all Alice in Wonderland and do the counting. You end up with:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
<pre>Next word after "Alice" → was: 17 times,  and: 16 times,  thought: 12 times,  had: 11 times, ...
Next word after "sat" → down: 9 times,  silent: 2 times,  still: 2 times,  for: 1 times, ...
Next word after "was" → a: 31 times,  the: 20 times,  not: 12 times,  going: 11 times, ...
Next word after "and" → the: 80 times,  she: 52 times,  then: 31 times,  was: 19 times, ...
Next word after "a" → little: 59 times,  very: 25 times,  large: 20 times,  great: 17 times
Next word after "very" → much: 10 times,  soon: 7 times,  curious: 6 times,  glad: 5 times
...
</pre>
</blockquote>

<p>This table gives you a lot of help in forming sentences in the style of Alice in Wonderland. 
For example, if you start with the word <em>Alice</em>, then you'd look that up in the above table
and see that the most frequent next word is <em>was</em>. And after <em>was</em> you would select <em>a</em>. After <em>a</em>
you would get <em>little</em>. You'd end up with <em>Alice was a little</em>.</p>
<p>How well does this work? Let's create some sentences:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
the door, when i beg your verdict," it was quite plainly through the bottle, i'm afraid that they had its nest.
</blockquote>

<p>You can try this out for yourself on <a href="http://almeopedia.com/markovtest.html">almeopedia</a>.</p>
<p>That's not great. Why is it so nonsensical? Think back to our fill in the blank examples at the start of 
this post: in order to pick a good next word you need context. If someone asked you what word should 
appear after <em>rock</em>, you'd have a hard time picking something reasonable, but if they gave you 
<em>rock and</em>, you'd fairly quickly think of <em>roll</em>.</p>
<p>What would happen if you considered two words instead of a single word for your context? 
For one thing your frequency table creation would become harder - now instead of a single line for
each word, you'd have a line for each word pair. You'd have to gather stats for all the two word 
permutations, which is a lot more than the single word case.</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
<pre>Next word after "Alice was" → not: 3 times,  beginning: 2 times,  very: 2 times, ...
Next word after "was not" → a: 3 times,  here: 1 times,  going: 1 times ...
Next word after "not here" → before: 1 times
Next word after "not a" → moment: 2 times,  bit: 2 times,  serpent: 2 times
</pre>
...
</blockquote>

<p>Let's create a sentence with this and see how it looks:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
the hatter, and, burning with curiosity, she decided on going into the garden.
</blockquote>

<p>You can try this out for yourself on <a href="http://www.almeopedia.com/markov3test.html">almeopedia</a>.</p>
<p>That's looking better, it almost makes sense. Let's keep going - instead of two words
of context, how about three? </p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
The Hatter’s remark seemed to have no sort of chance of her ever getting out of the water, and seemed to quiver all over with diamonds, and walked two and two, as the soldiers did.
</blockquote>

<p>Four?</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
So she swallowed one of the cakes, and was delighted to find that she knew the name of nearly everything there.
</blockquote>

<p>Hmm. This is a little too good. It turns out a very similar sentence exists in the original Alice in Wonderland text:</p>
<blockquote class="p-4 my-4 border-l-2 border-gray-300 bg-gray-200">
So she swallowed one of the cakes, and was delighted to find that she
began shrinking directly.
</blockquote>

<p>Our simple method of picking the most likely next word can result in the system memorizing the text snippets -
given long enough context, the next most likely word is exactly the word that appeared in the original text 
following that context. </p>
<p>We can fix this by introducing some randomness - instead of always picking the most likely next word, we can
pick somewhat likely next words. This way we're less likely to regurgitate the original text.</p>
<p>The good news is our method seems to work for English, so it'll probably work for alien cat language as well.</p>
<p><i>Aside: You might run into the term "stochastic" as you look into language models - this just means randomly determined.
For example, you might hear people argue whether these systems are <strong>Stochastic Parrots</strong>, implying the systems
are simply parroting back the original text that they saw, with some randomness thrown in.
</i></p>
<h1>Large Language Models</h1>
<p>ChatGPT and many other Large Language Models (LLMs) do essentially what you did above: they examine
a very large amount of human communication, gather stats (or probabilities) on what words are most likely 
to follow other words, and play a continous game of fill-in-the-blank. You give them some context (your prompt or
question), and they create the reply one word at a time by selecting the word most likely to appear next.
They respond with a word, then look at their internal stats to pick a word to follow their first word, then
a word to follow their second word, and so forth, one word at a time, until they've formed a response.</p>
<h1>To be Continued...</h1>
<p>In <a href="/a-non-technical-explanation-of-chatgpt-deep-learning.html">part two of this series</a> we'll look at the problems you'll run into with this method and how deep learning helps you overcome those problems.</p>
<h1>Further Reading</h1>
<p><em>If you're interested in this topic you might also enjoy the other posts in the <a href="/tag/explainer.html">Non-Technical Explainer</a> series as well.</em></p>
        </div>
    </section>
    <footer class="footer-article py-8">
        <div class="tags-and-categories"><span class="italic">Category: </span><a href="https://www.parand.com/category/machine-learning.html">Machine Learning</a>
| <span class="italic">Tags:</span> <a class="inline-block py-1 px-2 rounded bg-indigo-50 text-indigo-500 text-base font-medium tracking-widest" href="https://www.parand.com/tag/explainer.html">explainer </a><a class="inline-block py-1 px-2 rounded bg-indigo-50 text-indigo-500 text-base font-medium tracking-widest" href="https://www.parand.com/tag/machine-learning.html">machine-learning </a><a class="inline-block py-1 px-2 rounded bg-indigo-50 text-indigo-500 text-base font-medium tracking-widest" href="https://www.parand.com/tag/llm.html">llm </a>        </div>
    </footer>
    <!-- disqus  -->

  </div>
            </div>
            </div>
          </div>
        </div>
    </section>

</div>


</body>
</html>